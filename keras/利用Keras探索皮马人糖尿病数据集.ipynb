{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用Keras探索皮马人糖尿病数据集\n",
    "\n",
    "## 目录\n",
    "- [1.数据集介绍](#1.-数据集介绍)\n",
    "- [2.导入数据集](#2.-导入数据集)\n",
    "- [3.查看数据信息](#3.-查看数据信息)\n",
    "- [4.使用Keras建立神经网络](#4.-使用Keras建立神经网络)\n",
    "    - [4.1 定义模型](#4.1-定义模型)\n",
    "- [5.测试神经网络](#5.-测试神经网络)  \n",
    "    - [5.1 口算神经网络](#5.1-口算神经网络)\n",
    "    - [5.2 分割数据](#5.2-分割数据)\n",
    "        - [5.2.1 自动验证](#5.2.1-自动验证)\n",
    "        - [5.2.2 手工验证](#5.2.2-手工验证)\n",
    "        - [4.2.3 K折交叉验证](#5.2.3-手工K折交叉验证)\n",
    "- [6.使用Scikit-Learn调用Keras的模型](#6.-使用Scikit-Learn调用Keras的模型)\n",
    "    - [6.1 使用交叉验证检验深度学习模型](#6.1-使用交叉验证检验深度学习模型)\n",
    "    - [6.2 使用网格搜索调整深度学习模型的参数](#6.2-使用网格搜索调整深度学习模型的参数)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据集介绍\n",
    "该数据集涵盖了皮马人的医疗记录，以及过去5年内是否有糖尿病，所有的数据都以数字的形式呈现。需要解决的问题是，判断一个instance是否有糖尿病（是为1否为0）。这显然是一个**二分类问题**。该数据集中有8个属性及1个类别，表示如下：\n",
    "\n",
    "- 怀孕次数 --- Number of times pregnant\n",
    "- 2小时口服葡萄糖耐量试验中的血浆葡萄糖浓度 --- Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "    舒张压（毫米汞柱）--- Diastolic blood pressure (mm Hg)\n",
    "- 2小时血清胰岛素（mu U/ml) --- 2-Hour serum insulin (mu U/ml)\n",
    "- 三头肌皮褶厚度 (毫米) --- Triceps skin fold thickness (mm)\n",
    "- 体重指数（BMI）--- Body mass index (weight in kg/(height in m)^2)\n",
    "- 糖尿病血系功能 --- Diabetes pedigree function\n",
    "- 年龄（年）--- Age (years)\n",
    "- 类别：过去5年内是否有糖尿病 --- Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/pima-indians-diabetes.csv', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 查看数据信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.000</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1   2   3   4     5      6   7  8\n",
       "0  0    1   2   3   4   5.0  6.000   7  8\n",
       "1  6  148  72  35   0  33.6  0.627  50  1\n",
       "2  1   85  66  29   0  26.6  0.351  31  0\n",
       "3  8  183  64   0   0  23.3  0.672  32  1\n",
       "4  1   89  66  23  94  28.1  0.167  21  0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0           1           2           3           4           5  \\\n",
       "count  768.000000  768.000000  768.000000  768.000000  768.000000  768.000000   \n",
       "mean     3.845052  120.894531   69.105469   20.536458   79.799479   31.992578   \n",
       "std      3.369578   31.972618   19.355807   15.952218  115.244002    7.884160   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      1.000000   99.000000   62.000000    0.000000    0.000000   27.300000   \n",
       "50%      3.000000  117.000000   72.000000   23.000000   30.500000   32.000000   \n",
       "75%      6.000000  140.250000   80.000000   32.000000  127.250000   36.600000   \n",
       "max     17.000000  199.000000  122.000000   99.000000  846.000000   67.100000   \n",
       "\n",
       "                6           7           8  \n",
       "count  768.000000  768.000000  768.000000  \n",
       "mean     0.471876   33.240885    0.348958  \n",
       "std      0.331329   11.760232    0.476951  \n",
       "min      0.078000   21.000000    0.000000  \n",
       "25%      0.243750   24.000000    0.000000  \n",
       "50%      0.372500   29.000000    0.000000  \n",
       "75%      0.626250   41.000000    1.000000  \n",
       "max      2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      "0    768 non-null int64\n",
      "1    768 non-null int64\n",
      "2    768 non-null int64\n",
      "3    768 non-null int64\n",
      "4    768 non-null int64\n",
      "5    768 non-null float64\n",
      "6    768 non-null float64\n",
      "7    768 non-null int64\n",
      "8    768 non-null int64\n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 使用Keras建立神经网络\n",
    "\n",
    "### 4.1 定义模型\n",
    "**Keras的模型由层构成**：我们建立一个Sequential模型，一层层加入神经元。第一步是确定输入层的数目正确：在创建模型时用input_dim参数确定。例如，有8个输入变量，就设成8。\n",
    "\n",
    "隐层怎么设置？这个问题很难回答，需要慢慢试验。一般来说，如果网络够大，即使存在问题也不会有影响。这个例子里我们用3层全连接网络。\n",
    "\n",
    "全连接层用Dense类定义：第一个参数是本层神经元个数，然后是初始化方式和激活函数。这里的初始化方法是0到0.05的连续型均匀分布（uniform），Keras的默认方法也是这个。也可以用高斯分布进行初始化（normal）。\n",
    "\n",
    "前两层的激活函数是线性整流函数relu，最后一层的激活函数是S型函数sigmoid。之前大家喜欢用S型和正切函数，但现在线性整流函数效果更好。为了保证输出是0到1的概率数字，最后一层的激活函数是S型函数，这样映射到0.5的阈值函数也容易。前两个隐层分别有12和8个神经元，最后一层是1个神经元（是否有糖尿病）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用随机梯度下降时最好固定随机数种子，这样你的代码每次运行的结果都一致。这种做法在演示结果、比较算法或debug时特别有效。你可以随便选种子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=data.iloc[:,0:8]\n",
    "Y=data.iloc[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\n",
    "model.add(Dense(8, init='uniform', activation='relu'))\n",
    "model.add(Dense(1, init='normal', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "769/769 [==============================] - 0s 566us/step - loss: 0.2924 - acc: 0.8023\n",
      "Epoch 2/150\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.2781 - acc: 0.8218\n",
      "Epoch 3/150\n",
      "769/769 [==============================] - 0s 120us/step - loss: 0.2886 - acc: 0.7984\n",
      "Epoch 4/150\n",
      "769/769 [==============================] - 0s 120us/step - loss: 0.2703 - acc: 0.8101\n",
      "Epoch 5/150\n",
      "769/769 [==============================] - 0s 121us/step - loss: 0.2691 - acc: 0.8153\n",
      "Epoch 6/150\n",
      "769/769 [==============================] - 0s 126us/step - loss: 0.2794 - acc: 0.8075\n",
      "Epoch 7/150\n",
      "769/769 [==============================] - 0s 124us/step - loss: 0.2739 - acc: 0.8036\n",
      "Epoch 8/150\n",
      "769/769 [==============================] - 0s 120us/step - loss: 0.2842 - acc: 0.7958\n",
      "Epoch 9/150\n",
      "769/769 [==============================] - 0s 124us/step - loss: 0.2643 - acc: 0.8101\n",
      "Epoch 10/150\n",
      "769/769 [==============================] - 0s 124us/step - loss: 0.2796 - acc: 0.8036\n",
      "Epoch 11/150\n",
      "769/769 [==============================] - 0s 123us/step - loss: 0.2654 - acc: 0.8036\n",
      "Epoch 12/150\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.2708 - acc: 0.7984\n",
      "Epoch 13/150\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.2655 - acc: 0.8088\n",
      "Epoch 14/150\n",
      "769/769 [==============================] - 0s 124us/step - loss: 0.2715 - acc: 0.8088\n",
      "Epoch 15/150\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.2717 - acc: 0.8114\n",
      "Epoch 16/150\n",
      "769/769 [==============================] - 0s 119us/step - loss: 0.2529 - acc: 0.8257\n",
      "Epoch 17/150\n",
      "769/769 [==============================] - 0s 123us/step - loss: 0.2803 - acc: 0.7971\n",
      "Epoch 18/150\n",
      "769/769 [==============================] - 0s 123us/step - loss: 0.2615 - acc: 0.8218\n",
      "Epoch 19/150\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.2727 - acc: 0.8127\n",
      "Epoch 20/150\n",
      "769/769 [==============================] - 0s 126us/step - loss: 0.2731 - acc: 0.8153\n",
      "Epoch 21/150\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.2541 - acc: 0.8166\n",
      "Epoch 22/150\n",
      "769/769 [==============================] - 0s 126us/step - loss: 0.2611 - acc: 0.8088\n",
      "Epoch 23/150\n",
      "769/769 [==============================] - 0s 123us/step - loss: 0.2660 - acc: 0.8049\n",
      "Epoch 24/150\n",
      "769/769 [==============================] - 0s 125us/step - loss: 0.2511 - acc: 0.8088\n",
      "Epoch 25/150\n",
      "769/769 [==============================] - 0s 130us/step - loss: 0.2729 - acc: 0.8075\n",
      "Epoch 26/150\n",
      "769/769 [==============================] - 0s 120us/step - loss: 0.2649 - acc: 0.8153\n",
      "Epoch 27/150\n",
      "769/769 [==============================] - 0s 127us/step - loss: 0.2603 - acc: 0.8062\n",
      "Epoch 28/150\n",
      "769/769 [==============================] - 0s 133us/step - loss: 0.2697 - acc: 0.7984\n",
      "Epoch 29/150\n",
      "769/769 [==============================] - 0s 127us/step - loss: 0.2683 - acc: 0.8010\n",
      "Epoch 30/150\n",
      "769/769 [==============================] - 0s 124us/step - loss: 0.2625 - acc: 0.8062\n",
      "Epoch 31/150\n",
      "769/769 [==============================] - 0s 124us/step - loss: 0.2624 - acc: 0.8088\n",
      "Epoch 32/150\n",
      "769/769 [==============================] - 0s 130us/step - loss: 0.2614 - acc: 0.8127\n",
      "Epoch 33/150\n",
      "769/769 [==============================] - 0s 128us/step - loss: 0.2676 - acc: 0.8075\n",
      "Epoch 34/150\n",
      "769/769 [==============================] - 0s 124us/step - loss: 0.2850 - acc: 0.8010\n",
      "Epoch 35/150\n",
      "769/769 [==============================] - 0s 127us/step - loss: 0.2495 - acc: 0.8127\n",
      "Epoch 36/150\n",
      "769/769 [==============================] - 0s 126us/step - loss: 0.2582 - acc: 0.8101\n",
      "Epoch 37/150\n",
      "769/769 [==============================] - 0s 125us/step - loss: 0.2615 - acc: 0.8140\n",
      "Epoch 38/150\n",
      "769/769 [==============================] - 0s 124us/step - loss: 0.2530 - acc: 0.8101\n",
      "Epoch 39/150\n",
      "769/769 [==============================] - 0s 128us/step - loss: 0.2635 - acc: 0.8270\n",
      "Epoch 40/150\n",
      "769/769 [==============================] - 0s 123us/step - loss: 0.2671 - acc: 0.8062\n",
      "Epoch 41/150\n",
      "769/769 [==============================] - 0s 125us/step - loss: 0.2652 - acc: 0.7971\n",
      "Epoch 42/150\n",
      "769/769 [==============================] - 0s 129us/step - loss: 0.2699 - acc: 0.8075\n",
      "Epoch 43/150\n",
      "769/769 [==============================] - 0s 124us/step - loss: 0.2646 - acc: 0.8101\n",
      "Epoch 44/150\n",
      "769/769 [==============================] - 0s 125us/step - loss: 0.2658 - acc: 0.8179\n",
      "Epoch 45/150\n",
      "769/769 [==============================] - 0s 124us/step - loss: 0.2672 - acc: 0.8088\n",
      "Epoch 46/150\n",
      "769/769 [==============================] - 0s 125us/step - loss: 0.2629 - acc: 0.8075\n",
      "Epoch 47/150\n",
      "769/769 [==============================] - 0s 129us/step - loss: 0.2534 - acc: 0.8101\n",
      "Epoch 48/150\n",
      "769/769 [==============================] - 0s 127us/step - loss: 0.2640 - acc: 0.8036\n",
      "Epoch 49/150\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.2570 - acc: 0.8101\n",
      "Epoch 50/150\n",
      "769/769 [==============================] - 0s 128us/step - loss: 0.2592 - acc: 0.8101\n",
      "Epoch 51/150\n",
      "769/769 [==============================] - 0s 125us/step - loss: 0.2565 - acc: 0.8114\n",
      "Epoch 52/150\n",
      "769/769 [==============================] - 0s 125us/step - loss: 0.2687 - acc: 0.8049\n",
      "Epoch 53/150\n",
      "769/769 [==============================] - 0s 117us/step - loss: 0.2541 - acc: 0.8140\n",
      "Epoch 54/150\n",
      "769/769 [==============================] - 0s 134us/step - loss: 0.2517 - acc: 0.8166\n",
      "Epoch 55/150\n",
      "769/769 [==============================] - 0s 123us/step - loss: 0.2533 - acc: 0.8192\n",
      "Epoch 56/150\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.2515 - acc: 0.8231\n",
      "Epoch 57/150\n",
      "769/769 [==============================] - 0s 118us/step - loss: 0.2634 - acc: 0.8088\n",
      "Epoch 58/150\n",
      "769/769 [==============================] - 0s 123us/step - loss: 0.2573 - acc: 0.8114\n",
      "Epoch 59/150\n",
      "769/769 [==============================] - 0s 119us/step - loss: 0.2591 - acc: 0.8218\n",
      "Epoch 60/150\n",
      "769/769 [==============================] - 0s 123us/step - loss: 0.2590 - acc: 0.8153\n",
      "Epoch 61/150\n",
      "769/769 [==============================] - 0s 125us/step - loss: 0.2548 - acc: 0.8088\n",
      "Epoch 62/150\n",
      "769/769 [==============================] - 0s 127us/step - loss: 0.2948 - acc: 0.8010\n",
      "Epoch 63/150\n",
      "769/769 [==============================] - 0s 131us/step - loss: 0.2717 - acc: 0.8075\n",
      "Epoch 64/150\n",
      "769/769 [==============================] - 0s 118us/step - loss: 0.2694 - acc: 0.8153\n",
      "Epoch 65/150\n",
      "769/769 [==============================] - 0s 118us/step - loss: 0.2523 - acc: 0.8114\n",
      "Epoch 66/150\n",
      "769/769 [==============================] - 0s 113us/step - loss: 0.2707 - acc: 0.8023\n",
      "Epoch 67/150\n",
      "769/769 [==============================] - 0s 114us/step - loss: 0.2647 - acc: 0.8153\n",
      "Epoch 68/150\n",
      "769/769 [==============================] - 0s 119us/step - loss: 0.2652 - acc: 0.7997\n",
      "Epoch 69/150\n",
      "769/769 [==============================] - 0s 123us/step - loss: 0.2553 - acc: 0.8088\n",
      "Epoch 70/150\n",
      "769/769 [==============================] - 0s 124us/step - loss: 0.2593 - acc: 0.8166\n",
      "Epoch 71/150\n",
      "769/769 [==============================] - 0s 120us/step - loss: 0.2720 - acc: 0.8101\n",
      "Epoch 72/150\n",
      "769/769 [==============================] - 0s 124us/step - loss: 0.2505 - acc: 0.8244\n",
      "Epoch 73/150\n",
      "769/769 [==============================] - 0s 128us/step - loss: 0.2580 - acc: 0.8127\n",
      "Epoch 74/150\n",
      "769/769 [==============================] - 0s 125us/step - loss: 0.2477 - acc: 0.8192\n",
      "Epoch 75/150\n",
      "769/769 [==============================] - 0s 129us/step - loss: 0.2536 - acc: 0.8192\n",
      "Epoch 76/150\n",
      "769/769 [==============================] - 0s 123us/step - loss: 0.2660 - acc: 0.8088\n",
      "Epoch 77/150\n",
      "769/769 [==============================] - 0s 124us/step - loss: 0.2500 - acc: 0.8166\n",
      "Epoch 78/150\n",
      "769/769 [==============================] - 0s 125us/step - loss: 0.2592 - acc: 0.8179\n",
      "Epoch 79/150\n",
      "769/769 [==============================] - 0s 123us/step - loss: 0.2573 - acc: 0.8127\n",
      "Epoch 80/150\n",
      "769/769 [==============================] - 0s 126us/step - loss: 0.2505 - acc: 0.8127\n",
      "Epoch 81/150\n",
      "769/769 [==============================] - 0s 138us/step - loss: 0.2544 - acc: 0.8101\n",
      "Epoch 82/150\n",
      "769/769 [==============================] - 0s 128us/step - loss: 0.2623 - acc: 0.8088\n",
      "Epoch 83/150\n",
      "769/769 [==============================] - 0s 125us/step - loss: 0.2553 - acc: 0.8257\n",
      "Epoch 84/150\n",
      "769/769 [==============================] - 0s 117us/step - loss: 0.2513 - acc: 0.8088\n",
      "Epoch 85/150\n",
      "769/769 [==============================] - 0s 118us/step - loss: 0.2619 - acc: 0.8153\n",
      "Epoch 86/150\n",
      "769/769 [==============================] - 0s 116us/step - loss: 0.2533 - acc: 0.8166\n",
      "Epoch 87/150\n",
      "769/769 [==============================] - 0s 118us/step - loss: 0.2700 - acc: 0.8036\n",
      "Epoch 88/150\n",
      "769/769 [==============================] - 0s 118us/step - loss: 0.2525 - acc: 0.8231\n",
      "Epoch 89/150\n",
      "769/769 [==============================] - 0s 119us/step - loss: 0.2525 - acc: 0.8127\n",
      "Epoch 90/150\n",
      "769/769 [==============================] - 0s 115us/step - loss: 0.2619 - acc: 0.8075\n",
      "Epoch 91/150\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.2507 - acc: 0.8153\n",
      "Epoch 92/150\n",
      "769/769 [==============================] - 0s 118us/step - loss: 0.2682 - acc: 0.8101\n",
      "Epoch 93/150\n",
      "769/769 [==============================] - 0s 124us/step - loss: 0.2649 - acc: 0.8153\n",
      "Epoch 94/150\n",
      "769/769 [==============================] - 0s 126us/step - loss: 0.2578 - acc: 0.8062\n",
      "Epoch 95/150\n",
      "769/769 [==============================] - 0s 126us/step - loss: 0.2768 - acc: 0.8062\n",
      "Epoch 96/150\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.2626 - acc: 0.8062\n",
      "Epoch 97/150\n",
      "769/769 [==============================] - 0s 131us/step - loss: 0.2588 - acc: 0.8023\n",
      "Epoch 98/150\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.2653 - acc: 0.8062\n",
      "Epoch 99/150\n",
      "769/769 [==============================] - 0s 127us/step - loss: 0.2629 - acc: 0.8127\n",
      "Epoch 100/150\n",
      "769/769 [==============================] - 0s 120us/step - loss: 0.2610 - acc: 0.8205\n",
      "Epoch 101/150\n",
      "769/769 [==============================] - 0s 130us/step - loss: 0.2661 - acc: 0.8114\n",
      "Epoch 102/150\n",
      "769/769 [==============================] - 0s 126us/step - loss: 0.2565 - acc: 0.8075\n",
      "Epoch 103/150\n",
      "769/769 [==============================] - 0s 130us/step - loss: 0.2617 - acc: 0.8114\n",
      "Epoch 104/150\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.2590 - acc: 0.8205\n",
      "Epoch 105/150\n",
      "769/769 [==============================] - 0s 124us/step - loss: 0.2590 - acc: 0.8166\n",
      "Epoch 106/150\n",
      "769/769 [==============================] - 0s 130us/step - loss: 0.2610 - acc: 0.8192\n",
      "Epoch 107/150\n",
      "769/769 [==============================] - 0s 125us/step - loss: 0.2573 - acc: 0.8062\n",
      "Epoch 108/150\n",
      "769/769 [==============================] - 0s 126us/step - loss: 0.2610 - acc: 0.8205\n",
      "Epoch 109/150\n",
      "769/769 [==============================] - 0s 125us/step - loss: 0.2538 - acc: 0.8166\n",
      "Epoch 110/150\n",
      "769/769 [==============================] - 0s 125us/step - loss: 0.2593 - acc: 0.8088\n",
      "Epoch 111/150\n",
      "769/769 [==============================] - 0s 128us/step - loss: 0.2422 - acc: 0.8192\n",
      "Epoch 112/150\n",
      "769/769 [==============================] - 0s 126us/step - loss: 0.2581 - acc: 0.8114\n",
      "Epoch 113/150\n",
      "769/769 [==============================] - 0s 124us/step - loss: 0.2488 - acc: 0.8192\n",
      "Epoch 114/150\n",
      "769/769 [==============================] - 0s 129us/step - loss: 0.2357 - acc: 0.8244\n",
      "Epoch 115/150\n",
      "769/769 [==============================] - 0s 128us/step - loss: 0.2445 - acc: 0.8179\n",
      "Epoch 116/150\n",
      "769/769 [==============================] - 0s 128us/step - loss: 0.2425 - acc: 0.8166 0s - loss: 0.1024 - acc: 0.819\n",
      "Epoch 117/150\n",
      "769/769 [==============================] - 0s 129us/step - loss: 0.2443 - acc: 0.8192\n",
      "Epoch 118/150\n",
      "769/769 [==============================] - 0s 124us/step - loss: 0.2588 - acc: 0.8166\n",
      "Epoch 119/150\n",
      "769/769 [==============================] - 0s 117us/step - loss: 0.2568 - acc: 0.8075\n",
      "Epoch 120/150\n",
      "769/769 [==============================] - 0s 129us/step - loss: 0.2465 - acc: 0.8153\n",
      "Epoch 121/150\n",
      "769/769 [==============================] - 0s 124us/step - loss: 0.2465 - acc: 0.8127\n",
      "Epoch 122/150\n",
      "769/769 [==============================] - 0s 132us/step - loss: 0.2434 - acc: 0.8231\n",
      "Epoch 123/150\n",
      "769/769 [==============================] - 0s 125us/step - loss: 0.2535 - acc: 0.8127\n",
      "Epoch 124/150\n",
      "769/769 [==============================] - 0s 124us/step - loss: 0.2630 - acc: 0.8101\n",
      "Epoch 125/150\n",
      "769/769 [==============================] - 0s 123us/step - loss: 0.2554 - acc: 0.8166\n",
      "Epoch 126/150\n",
      "769/769 [==============================] - 0s 121us/step - loss: 0.2537 - acc: 0.8114\n",
      "Epoch 127/150\n",
      "769/769 [==============================] - 0s 123us/step - loss: 0.2611 - acc: 0.8114\n",
      "Epoch 128/150\n",
      "769/769 [==============================] - 0s 123us/step - loss: 0.2550 - acc: 0.8062\n",
      "Epoch 129/150\n",
      "769/769 [==============================] - 0s 126us/step - loss: 0.2521 - acc: 0.8140\n",
      "Epoch 130/150\n",
      "769/769 [==============================] - 0s 119us/step - loss: 0.2669 - acc: 0.8010\n",
      "Epoch 131/150\n",
      "769/769 [==============================] - 0s 124us/step - loss: 0.2624 - acc: 0.8010\n",
      "Epoch 132/150\n",
      "769/769 [==============================] - 0s 123us/step - loss: 0.2465 - acc: 0.8218\n",
      "Epoch 133/150\n",
      "769/769 [==============================] - 0s 124us/step - loss: 0.2493 - acc: 0.8101\n",
      "Epoch 134/150\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.2461 - acc: 0.8179\n",
      "Epoch 135/150\n",
      "769/769 [==============================] - 0s 124us/step - loss: 0.2435 - acc: 0.8218\n",
      "Epoch 136/150\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.2564 - acc: 0.8023\n",
      "Epoch 137/150\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.2505 - acc: 0.8179\n",
      "Epoch 138/150\n",
      "769/769 [==============================] - 0s 124us/step - loss: 0.2517 - acc: 0.8166\n",
      "Epoch 139/150\n",
      "769/769 [==============================] - 0s 125us/step - loss: 0.2549 - acc: 0.8127\n",
      "Epoch 140/150\n",
      "769/769 [==============================] - 0s 124us/step - loss: 0.2648 - acc: 0.8049\n",
      "Epoch 141/150\n",
      "769/769 [==============================] - 0s 121us/step - loss: 0.2493 - acc: 0.8101\n",
      "Epoch 142/150\n",
      "769/769 [==============================] - 0s 124us/step - loss: 0.2389 - acc: 0.8179\n",
      "Epoch 143/150\n",
      "769/769 [==============================] - 0s 126us/step - loss: 0.2513 - acc: 0.8192\n",
      "Epoch 144/150\n",
      "769/769 [==============================] - 0s 125us/step - loss: 0.2425 - acc: 0.8218\n",
      "Epoch 145/150\n",
      "769/769 [==============================] - 0s 120us/step - loss: 0.2484 - acc: 0.8205\n",
      "Epoch 146/150\n",
      "769/769 [==============================] - 0s 123us/step - loss: 0.2465 - acc: 0.8205\n",
      "Epoch 147/150\n",
      "769/769 [==============================] - 0s 124us/step - loss: 0.2534 - acc: 0.8127\n",
      "Epoch 148/150\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.2484 - acc: 0.8270\n",
      "Epoch 149/150\n",
      "769/769 [==============================] - 0s 127us/step - loss: 0.2544 - acc: 0.8062\n",
      "Epoch 150/150\n",
      "769/769 [==============================] - 0s 123us/step - loss: 0.2572 - acc: 0.8205\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12328c210>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, Y, nb_epoch=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们把测试数据拿出来检验一下模型的效果。注意这样不能测试在新数据的预测能力。应该将数据分成训练和测试集。\n",
    "\n",
    "调用模型的evaluation()方法，传入训练时的数据。输出是平均值，包括平均误差和其他的数据，例如准确度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769/769 [==============================] - 0s 75us/step\n",
      "acc: 82.575\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X,Y)\n",
    "print '%s: %.3f' % (model.metrics_names[1], scores[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 测试神经网络\n",
    "\n",
    "深度学习有很多参数要调：大部分都是拍脑袋的。所以测试特别重要：本章我们讨论几种测试方法。本章将：\n",
    "\n",
    "- 使用Keras进行自动验证\n",
    "- 使用Keras进行手工验证\n",
    "- 使用Keras进行K折交叉验证\n",
    "\n",
    "### 5.1 口算神经网络\n",
    "创建神经网络时有很多参数：很多时候可以从别人的网络上抄，但是最终还是需要一点点做实验。无论是网络的拓扑结构（层数、大小、每层类型）还是小参数（损失函数、激活函数、优化算法、训练次数）等。\n",
    "\n",
    "一般深度学习的数据集都很大，数据有几十万乃至几亿个。所以测试方法至关重要。\n",
    "\n",
    "### 5.2 分割数据\n",
    "数据量大和网络复杂会造成训练时间很长，所以需要将数据分成训练、测试或验证数据集。Keras提供两种办法：\n",
    "\n",
    "自动验证\n",
    "手工验证\n",
    "#### 5.2.1 自动验证\n",
    "Keras可以将数据自动分出一部分，每次训练后进行验证。在训练时用**validation_split**参数可以指定验证数据的比例，一般是总数据的20%或者33%。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769/769 [==============================] - 0s 17us/step\n",
      "acc: 81.274\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, Y, validation_split=0.33, nb_epoch=150, batch_size=10, verbose=False)\n",
    "scores = model.evaluate(X,Y)\n",
    "print '%s: %.3f' % (model.metrics_names[1], scores[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 手工验证\n",
    "Keras也可以手工进行验证。我们定义一个**train_test_split**函数，将数据分成2：1的测试和验证数据集。在调用fit()方法时需要加入**validation_data**参数作为验证数据，数组的项目分别是输入和输出数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769/769 [==============================] - 0s 17us/step\n",
      "acc: 82.965\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test,y_test), nb_epoch=150, batch_size=10, verbose=False)\n",
    "scores = model.evaluate(X,Y)\n",
    "print '%s: %.3f' % (model.metrics_names[1], scores[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3 手工K折交叉验证\n",
    "机器学习的金科玉律是K折验证，以验证模型对未来数据的预测能力。K折验证的方法是：将数据分成K组，留下1组验证，其他数据用作训练，直到每种分发的性能一致。\n",
    "\n",
    "深度学习一般不用交叉验证，因为对算力要求太高。例如，K折的次数一般是5或者10折：每组都需要训练并验证，训练时间成倍上升。然而，如果数据量小，交叉验证的效果更好，误差更小。\n",
    "\n",
    "scikit-learn有StratifiedKFold类，我们用它把数据分成10组。抽样方法是分层抽样，尽可能保证每组数据量一致。然后我们在每组上训练模型，使用verbose=0参数关闭每轮的输出。训练后，Keras会输出模型的性能，并存储模型。最终，Keras输出性能的平均值和标准差，为性能估算提供更准确的估计。\n",
    "\n",
    "**当然，这是种繁琐的做法。我们可以直接使用scikit-learn去调用keras完成K-fold validation.**\n",
    "\n",
    "## 6. 使用Scikit-Learn调用Keras的模型\n",
    "Keras为scikit-learn封装了KerasClassifier和KerasRegressor方便我们对模型进行调参。\n",
    "\n",
    "### 6.1 使用交叉验证检验深度学习模型\n",
    "Keras的KerasClassifier和KerasRegressor两个类接受build_fn参数，传入编译好的模型。我们加入nb_epoch=150和batch_size=10这两个参数：这两个参数会传入模型的fit()方法。我们用scikit-learn的StratifiedKFold类进行10折交叉验证，测试模型在未知数据的性能，并使用cross_val_score()函数检测模型，打印结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.6788 - acc: 0.6504\n",
      "257/257 [==============================] - 0s 1ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.6806 - acc: 0.6484\n",
      "257/257 [==============================] - 0s 1ms/step\n",
      "Epoch 1/1\n",
      "514/514 [==============================] - 1s 2ms/step - loss: 0.6768 - acc: 0.6420\n",
      "255/255 [==============================] - 0s 2ms/step\n",
      "results: [0.64980545 0.64980545 0.65098039]\n",
      "mean result: 0.6501970970798299\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\n",
    "    model.add(Dense(8, init='uniform', activation='relu'))\n",
    "    model.add(Dense(1,init='normal', activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "X=data.iloc[:,0:8]\n",
    "Y=data.iloc[:,8]\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, nb_epoch=150, batch_size=10)\n",
    "kfold = StratifiedKFold(y=Y, n_folds=3, shuffle=True, random_state=seed)\n",
    "\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print 'results:', results\n",
    "print 'mean result:', results.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 使用网格搜索调整深度学习模型的参数\n",
    "使用scikit-learn封装Keras的模型十分简单。进一步想：我们可以给fit()方法传入参数，KerasClassifier的build_fn方法也可以传入参数。可以利用这点进一步调整模型。\n",
    "\n",
    "我们用网格搜索测试不同参数的性能：create_model()函数可以传入optimizer和init参数，虽然都有默认值。那么我们可以用不同的优化算法和初始权重调整网络。具体说，我们希望搜索：\n",
    "\n",
    "- 优化算法：搜索权重的方法\n",
    "- 初始权重：初始化不同的网络\n",
    "- 训练次数：对模型训练的次数\n",
    "- 批次大小：每次训练的数据量\n",
    "- 所有的参数组成一个字典，传入scikit-learn的GridSearchCV类：GridSearchCV会对每组参数（2×3×3×3）进行训练，进行3折交叉检验。\n",
    "\n",
    "计算量巨大：耗时巨长。如果模型小还可以取一部分数据试试。比如我们这里使用的模型，网络和数据集都不大（1000个数据内，9个参数）。最后scikit-learn会输出最好的参数和模型，以及平均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 2.1937 - acc: 0.6250\n",
      "257/257 [==============================] - 0s 2ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 3ms/step - loss: 3.5229 - acc: 0.4366\n",
      "256/256 [==============================] - 0s 2ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 1s 2ms/step - loss: 1.6050 - acc: 0.5556\n",
      "256/256 [==============================] - 0s 2ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 1s 3ms/step - loss: 1.7555 - acc: 0.5664\n",
      "257/257 [==============================] - 0s 2ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 1s 3ms/step - loss: 10.2795 - acc: 0.3528\n",
      "256/256 [==============================] - 0s 2ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 1s 3ms/step - loss: 3.6850 - acc: 0.5380\n",
      "256/256 [==============================] - 0s 2ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 1s 3ms/step - loss: 5.3517 - acc: 0.6680\n",
      "257/257 [==============================] - 1s 2ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 1s 3ms/step - loss: 7.4982 - acc: 0.4386\n",
      "256/256 [==============================] - 1s 2ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 1s 3ms/step - loss: 1.7838 - acc: 0.5302\n",
      "256/256 [==============================] - 1s 2ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 1s 3ms/step - loss: 6.7545 - acc: 0.4590\n",
      "257/257 [==============================] - 1s 2ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 4ms/step - loss: 2.2323 - acc: 0.4815\n",
      "256/256 [==============================] - 1s 2ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 3ms/step - loss: 2.4704 - acc: 0.5283\n",
      "256/256 [==============================] - 1s 2ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 1s 3ms/step - loss: 3.4122 - acc: 0.6328\n",
      "257/257 [==============================] - 1s 2ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 1s 3ms/step - loss: 5.7050 - acc: 0.6452\n",
      "256/256 [==============================] - 1s 2ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 3ms/step - loss: 5.4465 - acc: 0.6394\n",
      "256/256 [==============================] - 1s 2ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 2s 3ms/step - loss: 2.0756 - acc: 0.6191\n",
      "257/257 [==============================] - 1s 2ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 3ms/step - loss: 3.7551 - acc: 0.5887\n",
      "256/256 [==============================] - 1s 2ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 3ms/step - loss: 3.2473 - acc: 0.4230\n",
      "256/256 [==============================] - 1s 2ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 2s 3ms/step - loss: 0.6692 - acc: 0.6641\n",
      "257/257 [==============================] - 1s 2ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 3ms/step - loss: 0.6788 - acc: 0.6452\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 3ms/step - loss: 0.6774 - acc: 0.6394\n",
      "256/256 [==============================] - 1s 2ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 2s 3ms/step - loss: 0.6799 - acc: 0.6484\n",
      "257/257 [==============================] - 1s 2ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 3ms/step - loss: 0.6855 - acc: 0.6433\n",
      "256/256 [==============================] - 1s 2ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 3ms/step - loss: 0.6785 - acc: 0.6374\n",
      "256/256 [==============================] - 1s 2ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 2s 3ms/step - loss: 0.6670 - acc: 0.6719\n",
      "257/257 [==============================] - 1s 2ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 3ms/step - loss: 0.6787 - acc: 0.6277\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 3ms/step - loss: 0.6898 - acc: 0.5965\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 2s 4ms/step - loss: 0.6774 - acc: 0.6387\n",
      "257/257 [==============================] - 1s 3ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 4ms/step - loss: 0.6818 - acc: 0.6452\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 4ms/step - loss: 0.6755 - acc: 0.6374\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 2s 4ms/step - loss: 0.6700 - acc: 0.6621\n",
      "257/257 [==============================] - 1s 3ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 4ms/step - loss: 0.6783 - acc: 0.6413\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 4ms/step - loss: 0.6789 - acc: 0.6257\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 2s 4ms/step - loss: 0.6840 - acc: 0.6230\n",
      "257/257 [==============================] - 1s 3ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 4ms/step - loss: 0.6850 - acc: 0.6452\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 4ms/step - loss: 0.6894 - acc: 0.6394\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 2s 4ms/step - loss: 0.6808 - acc: 0.6641\n",
      "257/257 [==============================] - 1s 3ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 4ms/step - loss: 0.6788 - acc: 0.6238\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 4ms/step - loss: 0.6781 - acc: 0.6335\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 2s 4ms/step - loss: 0.6726 - acc: 0.6328\n",
      "257/257 [==============================] - 1s 3ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 4ms/step - loss: 0.6767 - acc: 0.6452\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 4ms/step - loss: 0.6867 - acc: 0.6062\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 2s 4ms/step - loss: 0.6671 - acc: 0.6680\n",
      "257/257 [==============================] - 1s 3ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 4ms/step - loss: 0.6765 - acc: 0.6277\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 4ms/step - loss: 0.6690 - acc: 0.6374\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 2s 4ms/step - loss: 0.6716 - acc: 0.6426\n",
      "257/257 [==============================] - 1s 3ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 4ms/step - loss: 0.6793 - acc: 0.6218\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 4ms/step - loss: 0.6808 - acc: 0.6374\n",
      "256/256 [==============================] - 1s 4ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 2s 4ms/step - loss: 0.6622 - acc: 0.6582\n",
      "257/257 [==============================] - 1s 4ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 4ms/step - loss: 0.6770 - acc: 0.6296\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 4ms/step - loss: 0.6757 - acc: 0.6374\n",
      "256/256 [==============================] - 1s 4ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.6807 - acc: 0.6602\n",
      "257/257 [==============================] - 1s 4ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 5ms/step - loss: 0.6757 - acc: 0.6335\n",
      "256/256 [==============================] - 1s 4ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 5ms/step - loss: 0.6753 - acc: 0.6218\n",
      "256/256 [==============================] - 1s 4ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 2s 4ms/step - loss: 5.3517 - acc: 0.6680\n",
      "257/257 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 4ms/step - loss: 5.6946 - acc: 0.6452\n",
      "256/256 [==============================] - 1s 4ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 5ms/step - loss: 5.8061 - acc: 0.6374\n",
      "256/256 [==============================] - 1s 4ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 10.6218 - acc: 0.3281\n",
      "257/257 [==============================] - 1s 4ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 5ms/step - loss: 1.3061 - acc: 0.5867\n",
      "256/256 [==============================] - 1s 4ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 5ms/step - loss: 4.6867 - acc: 0.4230\n",
      "256/256 [==============================] - 1s 4ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 2.8459 - acc: 0.5000\n",
      "257/257 [==============================] - 1s 4ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 5ms/step - loss: 2.9081 - acc: 0.4561\n",
      "256/256 [==============================] - 1s 4ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 2s 5ms/step - loss: 2.7068 - acc: 0.5965\n",
      "256/256 [==============================] - 1s 4ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 5.3517 - acc: 0.6680\n",
      "257/257 [==============================] - 1s 4ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 5ms/step - loss: 4.7872 - acc: 0.6413\n",
      "256/256 [==============================] - 1s 4ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 5ms/step - loss: 4.8742 - acc: 0.3606\n",
      "256/256 [==============================] - 1s 4ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 10.6189 - acc: 0.3320\n",
      "257/257 [==============================] - 1s 4ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 5ms/step - loss: 1.9542 - acc: 0.5945\n",
      "256/256 [==============================] - 1s 4ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 5ms/step - loss: 3.1041 - acc: 0.4191\n",
      "256/256 [==============================] - 1s 4ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.6217 - acc: 0.6523\n",
      "257/257 [==============================] - 1s 4ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 5ms/step - loss: 10.2764 - acc: 0.3528\n",
      "256/256 [==============================] - 1s 4ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 5ms/step - loss: 2.5856 - acc: 0.6335\n",
      "256/256 [==============================] - 1s 4ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.6789 - acc: 0.6680\n",
      "257/257 [==============================] - 1s 4ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 5ms/step - loss: 0.6873 - acc: 0.6335\n",
      "256/256 [==============================] - 1s 4ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 5ms/step - loss: 0.6773 - acc: 0.6296\n",
      "256/256 [==============================] - 1s 5ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 0.6812 - acc: 0.6699\n",
      "257/257 [==============================] - 1s 4ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 5ms/step - loss: 0.6866 - acc: 0.6238\n",
      "256/256 [==============================] - 1s 5ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 6ms/step - loss: 0.6847 - acc: 0.6335\n",
      "256/256 [==============================] - 1s 5ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 0.6711 - acc: 0.6660\n",
      "257/257 [==============================] - 1s 5ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 6ms/step - loss: 0.6804 - acc: 0.6355\n",
      "256/256 [==============================] - 1s 4ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 5ms/step - loss: 0.6852 - acc: 0.6238\n",
      "256/256 [==============================] - 1s 5ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 0.6809 - acc: 0.6602\n",
      "257/257 [==============================] - 1s 5ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 6ms/step - loss: 0.6913 - acc: 0.6023\n",
      "256/256 [==============================] - 1s 5ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 6ms/step - loss: 0.6894 - acc: 0.6043\n",
      "256/256 [==============================] - 1s 5ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 0.6811 - acc: 0.6582\n",
      "257/257 [==============================] - 1s 5ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 6ms/step - loss: 0.6806 - acc: 0.6452\n",
      "256/256 [==============================] - 1s 5ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 6ms/step - loss: 0.6824 - acc: 0.6374\n",
      "256/256 [==============================] - 1s 5ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 0.6667 - acc: 0.6680\n",
      "257/257 [==============================] - 1s 5ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 6ms/step - loss: 0.6829 - acc: 0.6452\n",
      "256/256 [==============================] - 1s 5ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 6ms/step - loss: 0.6915 - acc: 0.5244\n",
      "256/256 [==============================] - 1s 5ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 0.6703 - acc: 0.6680\n",
      "257/257 [==============================] - 1s 5ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 6ms/step - loss: 0.6841 - acc: 0.6335\n",
      "256/256 [==============================] - 1s 6ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 6ms/step - loss: 0.6728 - acc: 0.6374\n",
      "256/256 [==============================] - 1s 5ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 7ms/step - loss: 0.6825 - acc: 0.5977\n",
      "257/257 [==============================] - 1s 6ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 7ms/step - loss: 0.6882 - acc: 0.6277\n",
      "256/256 [==============================] - 1s 5ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 7ms/step - loss: 0.6840 - acc: 0.6062\n",
      "256/256 [==============================] - 1s 5ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 7ms/step - loss: 0.6655 - acc: 0.6660\n",
      "257/257 [==============================] - 1s 5ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 6ms/step - loss: 0.6905 - acc: 0.5595\n",
      "256/256 [==============================] - 1s 5ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 6ms/step - loss: 0.6810 - acc: 0.6374\n",
      "256/256 [==============================] - 1s 5ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 7ms/step - loss: 0.6667 - acc: 0.6367\n",
      "257/257 [==============================] - 1s 6ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 7ms/step - loss: 0.6731 - acc: 0.6472\n",
      "256/256 [==============================] - 1s 5ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 7ms/step - loss: 0.6841 - acc: 0.6374\n",
      "256/256 [==============================] - 1s 6ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 7ms/step - loss: 0.6777 - acc: 0.6387\n",
      "257/257 [==============================] - 1s 6ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 4s 7ms/step - loss: 0.6844 - acc: 0.6277\n",
      "256/256 [==============================] - 1s 6ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 4s 7ms/step - loss: 0.6759 - acc: 0.6277\n",
      "256/256 [==============================] - 1s 5ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 7ms/step - loss: 0.6727 - acc: 0.6367\n",
      "257/257 [==============================] - 1s 5ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 7ms/step - loss: 0.6818 - acc: 0.6433\n",
      "256/256 [==============================] - 2s 6ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 4s 7ms/step - loss: 0.6893 - acc: 0.6082\n",
      "256/256 [==============================] - 1s 6ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 7ms/step - loss: 5.3517 - acc: 0.6680\n",
      "257/257 [==============================] - 1s 5ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 7ms/step - loss: 3.2354 - acc: 0.3645\n",
      "256/256 [==============================] - 1s 6ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 3s 7ms/step - loss: 4.6196 - acc: 0.5692\n",
      "256/256 [==============================] - 1s 6ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 4s 7ms/step - loss: 9.8583 - acc: 0.3320\n",
      "257/257 [==============================] - 1s 6ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 4s 7ms/step - loss: 3.2941 - acc: 0.5497\n",
      "256/256 [==============================] - 1s 6ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 4s 7ms/step - loss: 7.3399 - acc: 0.3587\n",
      "256/256 [==============================] - 2s 6ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 11ms/step - loss: 2.0467 - acc: 0.6484\n",
      "257/257 [==============================] - 2s 6ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 5s 10ms/step - loss: 2.9050 - acc: 0.5263\n",
      "256/256 [==============================] - 2s 6ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 4s 7ms/step - loss: 5.8311 - acc: 0.6374\n",
      "256/256 [==============================] - 2s 6ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 4s 8ms/step - loss: 3.9729 - acc: 0.3320\n",
      "257/257 [==============================] - 2s 6ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 5s 11ms/step - loss: 5.1686 - acc: 0.4191\n",
      "256/256 [==============================] - 4s 14ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 4s 8ms/step - loss: 5.0270 - acc: 0.5556\n",
      "256/256 [==============================] - 2s 6ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 4s 8ms/step - loss: 6.5165 - acc: 0.3398\n",
      "257/257 [==============================] - 2s 8ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 6s 12ms/step - loss: 5.6138 - acc: 0.6452\n",
      "256/256 [==============================] - 2s 7ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 4s 8ms/step - loss: 6.2128 - acc: 0.5205\n",
      "256/256 [==============================] - 2s 7ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 4s 9ms/step - loss: 3.1858 - acc: 0.6328\n",
      "257/257 [==============================] - 2s 6ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 5s 9ms/step - loss: 3.9268 - acc: 0.6140\n",
      "256/256 [==============================] - 2s 6ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 4s 8ms/step - loss: 4.6766 - acc: 0.5556\n",
      "256/256 [==============================] - 2s 7ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 4s 8ms/step - loss: 0.6850 - acc: 0.6426\n",
      "257/257 [==============================] - 2s 6ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 4s 8ms/step - loss: 0.6878 - acc: 0.6413\n",
      "256/256 [==============================] - 2s 6ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 4s 7ms/step - loss: 0.6865 - acc: 0.6277\n",
      "256/256 [==============================] - 2s 6ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 4s 8ms/step - loss: 0.6846 - acc: 0.6309\n",
      "257/257 [==============================] - 2s 8ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 4s 8ms/step - loss: 0.6878 - acc: 0.6296\n",
      "256/256 [==============================] - 2s 7ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 4s 8ms/step - loss: 0.6908 - acc: 0.6160\n",
      "256/256 [==============================] - 2s 7ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 9ms/step - loss: 0.6757 - acc: 0.6680\n",
      "257/257 [==============================] - 2s 7ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 4s 8ms/step - loss: 0.6912 - acc: 0.5497\n",
      "256/256 [==============================] - 2s 7ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 1s 3ms/step - loss: 0.6905 - acc: 0.6218\n",
      "256/256 [==============================] - 2s 8ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 4s 9ms/step - loss: 0.6914 - acc: 0.6230\n",
      "257/257 [==============================] - 2s 7ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 5s 9ms/step - loss: 0.6910 - acc: 0.5828\n",
      "256/256 [==============================] - 2s 7ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 4s 8ms/step - loss: 0.6911 - acc: 0.5536\n",
      "256/256 [==============================] - 2s 7ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 4s 8ms/step - loss: 0.6828 - acc: 0.6445\n",
      "257/257 [==============================] - 2s 7ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 4s 8ms/step - loss: 0.6813 - acc: 0.6452\n",
      "256/256 [==============================] - 2s 7ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 4s 8ms/step - loss: 0.6873 - acc: 0.6082\n",
      "256/256 [==============================] - 2s 7ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 4s 9ms/step - loss: 0.6848 - acc: 0.6582\n",
      "257/257 [==============================] - 2s 7ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 4s 8ms/step - loss: 0.6892 - acc: 0.6101\n",
      "256/256 [==============================] - 2s 7ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 4s 9ms/step - loss: 0.6878 - acc: 0.6238\n",
      "256/256 [==============================] - 2s 7ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 4s 9ms/step - loss: 0.6905 - acc: 0.5879\n",
      "257/257 [==============================] - 2s 7ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 7s 13ms/step - loss: 0.6865 - acc: 0.6277\n",
      "256/256 [==============================] - 2s 7ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 4s 9ms/step - loss: 0.6851 - acc: 0.6374\n",
      "256/256 [==============================] - 2s 8ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 9ms/step - loss: 0.6762 - acc: 0.6680\n",
      "257/257 [==============================] - 2s 8ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 5s 9ms/step - loss: 0.6781 - acc: 0.6394\n",
      "256/256 [==============================] - 2s 8ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 5s 10ms/step - loss: 0.6856 - acc: 0.6374\n",
      "256/256 [==============================] - 2s 8ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 9ms/step - loss: 0.6730 - acc: 0.6738\n",
      "257/257 [==============================] - 2s 8ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 4s 9ms/step - loss: 0.6822 - acc: 0.6277\n",
      "256/256 [==============================] - 2s 8ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 4s 9ms/step - loss: 0.6879 - acc: 0.5887\n",
      "256/256 [==============================] - 2s 8ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 9ms/step - loss: 0.6825 - acc: 0.6270\n",
      "257/257 [==============================] - 2s 8ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 5s 10ms/step - loss: 0.6854 - acc: 0.6199\n",
      "256/256 [==============================] - 2s 8ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 5s 9ms/step - loss: 0.6875 - acc: 0.6374\n",
      "256/256 [==============================] - 2s 8ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 4s 9ms/step - loss: 0.6697 - acc: 0.6680\n",
      "257/257 [==============================] - 2s 8ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 4s 9ms/step - loss: 0.6866 - acc: 0.6062\n",
      "256/256 [==============================] - 2s 8ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 5s 9ms/step - loss: 0.6875 - acc: 0.6004\n",
      "256/256 [==============================] - 2s 8ms/step\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 9ms/step - loss: 0.6871 - acc: 0.5898\n",
      "257/257 [==============================] - 2s 8ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 5s 9ms/step - loss: 0.6817 - acc: 0.6257\n",
      "256/256 [==============================] - 2s 7ms/step\n",
      "Epoch 1/1\n",
      "513/513 [==============================] - 5s 9ms/step - loss: 0.6841 - acc: 0.6082\n",
      "256/256 [==============================] - 2s 8ms/step\n",
      "Epoch 1/1\n",
      "769/769 [==============================] - 5s 6ms/step - loss: 0.6814 - acc: 0.6502\n",
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x134c56390>,\n",
      "       fit_params={}, iid=True, n_jobs=1,\n",
      "       param_grid={'init': ['glorot_uniform', 'uniform', 'normal'], 'optimizer': ['rmsprop', 'adam'], 'nb_epoch': [50, 100, 150], 'batch_size': [5, 10, 20]},\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, init=init, activation='relu'))\n",
    "    model.add(Dense(8, init=init, activation='relu'))\n",
    "    model.add(Dense(1, init=init, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "X=data.iloc[:,0:8]\n",
    "Y=data.iloc[:,8]   \n",
    "\n",
    "model = KerasClassifier(build_fn=create_model)\n",
    "\n",
    "optimizers = ['rmsprop', 'adam']\n",
    "inits = ['glorot_uniform', 'uniform', 'normal']\n",
    "epoches = [50, 100, 150]\n",
    "batches = [5, 10, 20]\n",
    "\n",
    "param_grid= dict(optimizer=optimizers, nb_epoch=epoches, batch_size=batches, init=inits)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "results = grid.fit(X, Y)\n",
    "\n",
    "print results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.659298 using {'init': 'normal', 'optimizer': 'adam', 'nb_epoch': 100, 'batch_size': 20}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best: %f using %s\" % (results.best_score_, results.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上面的计算，我们得到了最佳模型的参数:\n",
    "- Best: 0.659298 using {'init': 'normal', 'optimizer': 'adam', 'nb_epoch': 100, 'batch_size': 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.637165 (0.039419) with: {'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'nb_epoch': 50, 'batch_size': 5}\n",
      "0.488849 (0.105365) with: {'init': 'glorot_uniform', 'optimizer': 'adam', 'nb_epoch': 50, 'batch_size': 5}\n",
      "0.586439 (0.047494) with: {'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'nb_epoch': 100, 'batch_size': 5}\n",
      "0.529249 (0.024535) with: {'init': 'glorot_uniform', 'optimizer': 'adam', 'nb_epoch': 100, 'batch_size': 5}\n",
      "0.638527 (0.020554) with: {'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'nb_epoch': 150, 'batch_size': 5}\n",
      "0.520073 (0.045074) with: {'init': 'glorot_uniform', 'optimizer': 'adam', 'nb_epoch': 150, 'batch_size': 5}\n",
      "0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'rmsprop', 'nb_epoch': 50, 'batch_size': 5}\n",
      "0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'adam', 'nb_epoch': 50, 'batch_size': 5}\n",
      "0.654147 (0.029988) with: {'init': 'uniform', 'optimizer': 'rmsprop', 'nb_epoch': 100, 'batch_size': 5}\n",
      "0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'adam', 'nb_epoch': 100, 'batch_size': 5}\n",
      "0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'rmsprop', 'nb_epoch': 150, 'batch_size': 5}\n",
      "0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'adam', 'nb_epoch': 150, 'batch_size': 5}\n",
      "0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'rmsprop', 'nb_epoch': 50, 'batch_size': 5}\n",
      "0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'adam', 'nb_epoch': 50, 'batch_size': 5}\n",
      "0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'rmsprop', 'nb_epoch': 100, 'batch_size': 5}\n",
      "0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'adam', 'nb_epoch': 100, 'batch_size': 5}\n",
      "0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'rmsprop', 'nb_epoch': 150, 'batch_size': 5}\n",
      "0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'adam', 'nb_epoch': 150, 'batch_size': 5}\n",
      "0.650241 (0.025869) with: {'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'nb_epoch': 50, 'batch_size': 10}\n",
      "0.487785 (0.077297) with: {'init': 'glorot_uniform', 'optimizer': 'adam', 'nb_epoch': 50, 'batch_size': 10}\n",
      "0.574842 (0.037923) with: {'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'nb_epoch': 100, 'batch_size': 10}\n",
      "0.520033 (0.138485) with: {'init': 'glorot_uniform', 'optimizer': 'adam', 'nb_epoch': 100, 'batch_size': 10}\n",
      "0.538566 (0.111370) with: {'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'nb_epoch': 150, 'batch_size': 10}\n",
      "0.510898 (0.124085) with: {'init': 'glorot_uniform', 'optimizer': 'adam', 'nb_epoch': 150, 'batch_size': 10}\n",
      "0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'rmsprop', 'nb_epoch': 50, 'batch_size': 10}\n",
      "0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'adam', 'nb_epoch': 50, 'batch_size': 10}\n",
      "0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'rmsprop', 'nb_epoch': 100, 'batch_size': 10}\n",
      "0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'adam', 'nb_epoch': 100, 'batch_size': 10}\n",
      "0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'rmsprop', 'nb_epoch': 150, 'batch_size': 10}\n",
      "0.652845 (0.028562) with: {'init': 'uniform', 'optimizer': 'adam', 'nb_epoch': 150, 'batch_size': 10}\n",
      "0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'rmsprop', 'nb_epoch': 50, 'batch_size': 10}\n",
      "0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'adam', 'nb_epoch': 50, 'batch_size': 10}\n",
      "0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'rmsprop', 'nb_epoch': 100, 'batch_size': 10}\n",
      "0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'adam', 'nb_epoch': 100, 'batch_size': 10}\n",
      "0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'rmsprop', 'nb_epoch': 150, 'batch_size': 10}\n",
      "0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'adam', 'nb_epoch': 150, 'batch_size': 10}\n",
      "0.512220 (0.092497) with: {'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'nb_epoch': 50, 'batch_size': 20}\n",
      "0.478670 (0.110245) with: {'init': 'glorot_uniform', 'optimizer': 'adam', 'nb_epoch': 50, 'batch_size': 20}\n",
      "0.605940 (0.073768) with: {'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'nb_epoch': 100, 'batch_size': 20}\n",
      "0.503410 (0.086696) with: {'init': 'glorot_uniform', 'optimizer': 'adam', 'nb_epoch': 100, 'batch_size': 20}\n",
      "0.603503 (0.066787) with: {'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'nb_epoch': 150, 'batch_size': 20}\n",
      "0.563078 (0.076691) with: {'init': 'glorot_uniform', 'optimizer': 'adam', 'nb_epoch': 150, 'batch_size': 20}\n",
      "0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'rmsprop', 'nb_epoch': 50, 'batch_size': 20}\n",
      "0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'adam', 'nb_epoch': 50, 'batch_size': 20}\n",
      "0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'rmsprop', 'nb_epoch': 100, 'batch_size': 20}\n",
      "0.654132 (0.020581) with: {'init': 'uniform', 'optimizer': 'adam', 'nb_epoch': 100, 'batch_size': 20}\n",
      "0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'rmsprop', 'nb_epoch': 150, 'batch_size': 20}\n",
      "0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'adam', 'nb_epoch': 150, 'batch_size': 20}\n",
      "0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'rmsprop', 'nb_epoch': 50, 'batch_size': 20}\n",
      "0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'adam', 'nb_epoch': 50, 'batch_size': 20}\n",
      "0.651543 (0.027187) with: {'init': 'normal', 'optimizer': 'rmsprop', 'nb_epoch': 100, 'batch_size': 20}\n",
      "0.659356 (0.031877) with: {'init': 'normal', 'optimizer': 'adam', 'nb_epoch': 100, 'batch_size': 20}\n",
      "0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'rmsprop', 'nb_epoch': 150, 'batch_size': 20}\n",
      "0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'adam', 'nb_epoch': 150, 'batch_size': 20}\n"
     ]
    }
   ],
   "source": [
    "for params, mean_score, scores in results.grid_scores_:\n",
    "    print(\"%f (%f) with: %r\" % (scores.mean(), scores.std(), params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769/769 [==============================] - 0s 67us/step\n",
      "acc: 72.432\n"
     ]
    }
   ],
   "source": [
    "# st: 0.659298 using {'init': 'normal', 'optimizer': 'adam', 'nb_epoch': 100, 'batch_size': 20}\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, init='normal', activation='relu'))\n",
    "model.add(Dense(8, init='normal', activation='relu'))\n",
    "model.add(Dense(1, init='normal', activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.33, random_state=seed)\n",
    "model.fit(X_train, y_train, validation_data=(X_test,y_test), nb_epoch=100, batch_size=20, verbose=False)\n",
    "scores = model.evaluate(X,Y)\n",
    "print '%s: %.3f' % (model.metrics_names[1], scores[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
